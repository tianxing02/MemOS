import torch

from PIL import Image
from transformers import AutoModel, AutoTokenizer


def init_model(cache_path):
    model_path = (
        cache_path
        if (cache_path is not None and cache_path != "None")
        else "openbmb/MiniCPM-Llama3-V-2_5"
    )
    model = AutoModel.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        device_map="auto",
    ).eval()

    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model.tokenizer = tokenizer
    return model


def get_response_concat(model, question, image_path_list, max_new_tokens=1024, temperature=1.0):
    msgs = []
    system_prompt = "Answer in detail."
    if system_prompt:
        msgs.append({"type": "text", "value": system_prompt})
    if isinstance(image_path_list, list):
        msgs.extend([{"type": "image", "value": p} for p in image_path_list])
    else:
        msgs = [{"type": "image", "value": image_path_list}]
    msgs.append({"type": "text", "value": question})

    content = []
    for x in msgs:
        if x["type"] == "text":
            content.append(x["value"])
        elif x["type"] == "image":
            image = Image.open(x["value"]).convert("RGB")
            content.append(image)
    msgs = [{"role": "user", "content": content}]

    with torch.cuda.amp.autocast():
        res = model.chat(
            msgs=msgs,
            context=None,
            image=None,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature != 0.0,
            tokenizer=model.tokenizer,
        )
    return res
